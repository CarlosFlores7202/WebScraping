from bs4 import BeautifulSoup
import pytz
import requests
import pandas as pd
from datetime import datetime, timedelta

# zona horaria - Fecha inicio - Dias - Fecha Fin -

peru_timezone = pytz.timezone('America/Lima')
fechainicio = datetime(2019, 1, 1)
numero_dias = 30

if numero_dias > 0:
    fechafin = fechainicio + timedelta(days=numero_dias - 1)
else:
    fechafin = fechainicio

# Lista para cada pagina 
dfs = []

# Bucle para las fechas
while fechainicio <= fechafin:
  
    fechafinal = fechainicio.strftime("%Y-%m-%d")

    
    urlbusqueda = f"https://elcomercio.pe/archivo/todas/{fechafinal}/"

    # Realiza la solicitud HTTP
    html = requests.get(urlbusqueda)
    bsobj = BeautifulSoup(html.content, 'lxml')

    # Listas
    Titulo = []
    Fecha = []
    links = []

    # titulo
    for link in bsobj.findAll("h2"):
        Titulo.append(link.text)
    #Fecha
    for news in bsobj.find_all('p', {'class': 'story-item__date font-thin ml-5 text-xs text-gray-300 md:mt-5 md:ml-0'}):
        Fecha.append(news.text.strip())

    #Link
    for index, news in enumerate(bsobj.find_all('a', {'class': 'story-item__title block overflow-hidden primary-font line-h-xs mt-10'})):
        link_new = news.get('href')
        if link_new:
            full_link = "https://elcomercio.pe" + link_new
            links.append(full_link)

        else:
            link_text = "No tiene link"
            links.append(link_text)

    print(len(Titulo))
    print(len(Fecha))
    print(len(links))

    # dataframe
    df = pd.DataFrame({
        'Titulo': Titulo,
        'Fecha': Fecha,
        'Link': links,
    })

    # Agregamos la pag (bucle)
    dfs.append(df)

    fechainicio += timedelta(days=1)

# union
dataframefinal = pd.concat(dfs, ignore_index=True)
pd.set_option('display.max_rows', None)


print(dataframefinal)

# Guardado
dataframefinal.to_excel('elcomercio_data3.xlsx', index=False)
dataframefinal.to_csv('elcomercio_data3.csv', index=False)
